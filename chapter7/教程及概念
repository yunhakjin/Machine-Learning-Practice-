AdaBoost 元算法：
    将不同的分类器组合起来，组合结果被称为集成方法或者元算法
    集成方法形式可以是： 不同算法的集成，也可以是同一种算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成
AdaBoost:
    优点：泛化错误低，易编码，可以应用在大部分分类器上，无参数调整
    缺点：对离群点敏感
    适用数据类型：数值型和标称型数据
bagging方法：
     自举汇聚法(bootstrap aggregating)，也称bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原数据集的大小相等。
     每个数据集都是通过在原始数据集中随机选择一个样本来进行替换得到的。这里的替换就意味着可以多次选择同一个样本。这一性质就允许新数据集中可以有重复的值
     而原始数据集的某些之在新集合中则不会再出现。
     再S个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了S个分类器。当我们要对新数据进行分类时，就可以应用这S个分类器进行分类。与此同时，
     选择分类器投票结果中最多的类别作为最后的分类结果。
     进阶演化的算法：随机森林
boosting方法：
     boosting是一种与bagging很类似的方法。不论是再boosting还是bagging当中，所使用的多个分类器的类型都是一致的。但是再前者中，不同的分类器是通过穿行训练获得的
     ，每个新分类器都根据已训练出的分类器性能来进行训练。boosting是通过集中关注被已有的分类器错分的那些数据来获得新的分类器。
     由于boosting的分类结果是基于所有分类器的加权求和结果的，因此boosting和bagging不太一样。bagging中的分类器权重是相等的，而boosting中的分类器权重并不相等，
     每个权重代表的是其对应分类器在上一轮迭代中的成功度。
     boosting的最流行版本：AdaBoost
Adaboost的训练算法：
     训练数据中的每一个样本，并赋予其一个权重，这些权重构成了向量D。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，
     然后在同一数据机上再次训练弱分类器。在分类器的第二次训练中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会
     提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行进行计算的。
     (σ)错误率=未正确分类的样本数目/所有样本数目
     α=1/2*ln((1-σ)/σ)
     计算出α值之后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。